{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f99280915764a5f926fd3e602891f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a5472d58d8bf473b81469a47f7297fab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_95497e88c6354c7cb0b0c31ceb81557a",
              "IPY_MODEL_ba4ee6b4c79c498085dead4031daa319"
            ]
          }
        },
        "a5472d58d8bf473b81469a47f7297fab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95497e88c6354c7cb0b0c31ceb81557a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a029766266c54f4bae759fc0cf5ded60",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1464a72646204b5782429395e55ce759"
          }
        },
        "ba4ee6b4c79c498085dead4031daa319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ee1f43b2cb464172a2948b6d1c3165b0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 2.91kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_98f8fb59f90e438fbb65ff998b4082ea"
          }
        },
        "a029766266c54f4bae759fc0cf5ded60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1464a72646204b5782429395e55ce759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee1f43b2cb464172a2948b6d1c3165b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "98f8fb59f90e438fbb65ff998b4082ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aae6e2080d80420ca368a8053b449e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd2f8bf429e7489a91079d956cb5784b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2facad8b6fa34bc69908652c483feceb",
              "IPY_MODEL_cd4d5d4ff61549238887e988971bc4f0"
            ]
          }
        },
        "bd2f8bf429e7489a91079d956cb5784b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2facad8b6fa34bc69908652c483feceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_749a53ebc4924484a55f3f9d88d5f68a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f1f6c6f864b47cbbe739f07fc7c81a5"
          }
        },
        "cd4d5d4ff61549238887e988971bc4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ce461f45f35d4441b7332706d302bc03",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:08&lt;00:00, 50.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b0cbf3577275443f82a5db2401da4e38"
          }
        },
        "749a53ebc4924484a55f3f9d88d5f68a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f1f6c6f864b47cbbe739f07fc7c81a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce461f45f35d4441b7332706d302bc03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b0cbf3577275443f82a5db2401da4e38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6CyImjelhBt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "43c3e532-5b8f-4e8a-f913-e80eaf56f7a5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WIdwW53muEK",
        "colab_type": "text"
      },
      "source": [
        "## (Most) Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGcqDxobl_OI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#standard imports\n",
        "import datetime\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "#3rd party imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "\n",
        "#local app & library specific imports\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, random_split, RandomSampler, SequentialSampler, TensorDataset\n",
        "from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDyvuUOpmOII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrBUScizmz2o",
        "colab_type": "text"
      },
      "source": [
        "## Setup & global vars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY5vNTOtlsU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b26c7c9c-5a52-4522-8940-0b9ff54d49d2"
      },
      "source": [
        "%cd drive/'My Drive'/propaganda_bert/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/propaganda_bert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9GwaCdJl333",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4a7ca65c-46c1-459c-a7ea-e6f3970963ea"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR0hNRJdl7iA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1699a50f-3d2a-4193-ca0f-03ab55fa7166"
      },
      "source": [
        "# Load the dataset into a pandas dataframe.\n",
        "train_df = pd.read_csv(\n",
        "    \"datasets/train_data.tsv\",\n",
        "    sep = \"\\t\",\n",
        "    header = 0,\n",
        "    index_col = 0\n",
        ")\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(train_df.shape[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 15,928\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbdUfpFvmWXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "99b7dae2-2fd5-4f9c-c427-4daf454674f9"
      },
      "source": [
        "train_df.sample(3)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>label_binary</th>\n",
              "      <th>rand_letter</th>\n",
              "      <th>text</th>\n",
              "      <th>label_encoded</th>\n",
              "      <th>label_binary_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15441</th>\n",
              "      <td>No_Propaganda</td>\n",
              "      <td>No_Propaganda</td>\n",
              "      <td>s</td>\n",
              "      <td>attempt, that “the US bishops gave themselves ...</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6826</th>\n",
              "      <td>Loaded_Language</td>\n",
              "      <td>Propaganda</td>\n",
              "      <td>s</td>\n",
              "      <td>forced</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7577</th>\n",
              "      <td>No_Propaganda</td>\n",
              "      <td>No_Propaganda</td>\n",
              "      <td>s</td>\n",
              "      <td>Executive Grants of Clemency (Full Pardons) fo...</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 label   label_binary  ... label_encoded label_binary_encoded\n",
              "15441    No_Propaganda  No_Propaganda  ...            10                    0\n",
              "6826   Loaded_Language     Propaganda  ...             8                    1\n",
              "7577     No_Propaganda  No_Propaganda  ...            10                    0\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jajcAhXdo0xF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "341a90a4-24a2-4fdd-9265-63efe88e1277"
      },
      "source": [
        "#Global vars - vars in ALL_CAPS can be adjusted to enable experiments\n",
        "\n",
        "#Dataset vars\n",
        "THESE_LABELS = \"label\"\n",
        "NUM_LABELS = train_df[THESE_LABELS].nunique()\n",
        "print(f\"NUM_LABELS: {NUM_LABELS}\")\n",
        "\n",
        "#BERT/Training vars\n",
        "MODEL = \"bert-base-cased\"\n",
        "EPOCHS = 3 #A good starting range is 2 to 4\n",
        "LEARNING_RATE = 3e-5\n",
        "BATCH_SIZE = 32 #For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32."
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NUM_LABELS: 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc-SwcvepYhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = train_df.text.values\n",
        "\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(train_df[THESE_LABELS]) #encode labels as ints\n",
        "\n",
        "train_df[THESE_LABELS+\"_encoded\"] = labels #add col to train_df with ints for labels"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNwugkqkpcRz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "058c33b9-526d-4af0-9b61-22e990e2a242"
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "if MODEL == \"distilbert-base-uncased\":\n",
        "\n",
        "  from transformers import DistilBertTokenizer\n",
        "  tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case = True)\n",
        "\n",
        "elif MODEL == \"bert-base-cased\":\n",
        "\n",
        "  from transformers import BertTokenizer\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case = False)\n",
        "\n",
        "else:\n",
        "  raise ValueError('Unknown model specified. Check MODEL var.')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJUNvf_8qaqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c1389a36-a066-4c48-d563-4316e528e842"
      },
      "source": [
        "random_sample = random.randint(0,train_df.shape[0])\n",
        "\n",
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[random_sample])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[random_sample]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[random_sample])))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  how badly Trump does his job\n",
            "Tokenized:  ['how', 'badly', 'Trump', 'does', 'his', 'job']\n",
            "Token IDs:  [1293, 6118, 8499, 1674, 1117, 2261]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-whFHrRxqpm7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "387d823a-6ea1-45e7-de90-05e9f9f2c3b1"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "if max_len <= 512:\n",
        "  print('Max sentence length: ', max_len) #max sentence len for BERT is 512\n",
        "else:\n",
        "  warnings.warn(\"WARNING: max_len exceeds max token length for BERT.\")"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecRa9_OArWZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "abfa5b25-10b4-4edf-d648-23af0a7c464a"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    #`encode_plus` will:\n",
        "    #(1) Tokenize the sentence.\n",
        "    #(2) Prepend the `[CLS]` token to the start.\n",
        "    #(3) Append the `[SEP]` token to the end.\n",
        "    #(4) Map tokens to their IDs.\n",
        "    #(5) Pad or truncate the sentence to `max_length`\n",
        "    #(6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        sent,                         #Sentence to encode.\n",
        "        add_special_tokens = True,    #Add '[CLS]' and '[SEP]'\n",
        "        max_length = max_len,         #Pad & truncate all sentences.\n",
        "        truncation = True,\n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True, # Construct attn. masks.\n",
        "        return_tensors = 'pt',        # Return pytorch tensors.\n",
        "    )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence as a list of IDs.\n",
        "print('Original: ', sentences[random_sample])\n",
        "print('Token IDs:', input_ids[random_sample])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  how badly Trump does his job\n",
            "Token IDs: tensor([ 101, 1293, 6118, 8499, 1674, 1117, 2261,  102,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xER2TbP-rqGN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ed2269d6-6368-4585-b089-0eb5e199b1f2"
      },
      "source": [
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14,335 training samples\n",
            "1,593 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHVTSNSVtTsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = BATCH_SIZE # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = BATCH_SIZE # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWq_YCbLuEja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9f99280915764a5f926fd3e602891f7a",
            "a5472d58d8bf473b81469a47f7297fab",
            "95497e88c6354c7cb0b0c31ceb81557a",
            "ba4ee6b4c79c498085dead4031daa319",
            "a029766266c54f4bae759fc0cf5ded60",
            "1464a72646204b5782429395e55ce759",
            "ee1f43b2cb464172a2948b6d1c3165b0",
            "98f8fb59f90e438fbb65ff998b4082ea",
            "aae6e2080d80420ca368a8053b449e51",
            "bd2f8bf429e7489a91079d956cb5784b",
            "2facad8b6fa34bc69908652c483feceb",
            "cd4d5d4ff61549238887e988971bc4f0",
            "749a53ebc4924484a55f3f9d88d5f68a",
            "6f1f6c6f864b47cbbe739f07fc7c81a5",
            "ce461f45f35d4441b7332706d302bc03",
            "b0cbf3577275443f82a5db2401da4e38"
          ]
        },
        "outputId": "e336ae86-5459-4c28-c3f2-be36e2ed1680"
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top.\n",
        "if MODEL == \"distilbert-base-uncased\":\n",
        "\n",
        "  from transformers import DistilBertForSequenceClassification\n",
        "\n",
        "  model = DistilBertForSequenceClassification.from_pretrained(\n",
        "      \"distilbert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = NUM_LABELS, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "  )\n",
        "elif MODEL == \"bert-base-cased\":\n",
        "\n",
        "  from transformers import BertForSequenceClassification\n",
        "\n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = NUM_LABELS, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "else:\n",
        "  raise ValueError('Unknown model specified. Check MODEL var.')\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f99280915764a5f926fd3e602891f7a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aae6e2080d80420ca368a8053b449e51",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3w5c_jBuScV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr = LEARNING_RATE, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        ")"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h26i5Cgufhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = EPOCHS\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "    num_training_steps = total_steps\n",
        ")"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzFDyuQRuzXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIeS_U-4u3_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hilCF5eIu6e8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9b6df54-5f94-459a-af6b-7ca576e1ba32"
      },
      "source": [
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    448.    Elapsed: 0:00:44.\n",
            "  Batch    80  of    448.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    448.    Elapsed: 0:02:13.\n",
            "  Batch   160  of    448.    Elapsed: 0:02:58.\n",
            "  Batch   200  of    448.    Elapsed: 0:03:43.\n",
            "  Batch   240  of    448.    Elapsed: 0:04:28.\n",
            "  Batch   280  of    448.    Elapsed: 0:05:13.\n",
            "  Batch   320  of    448.    Elapsed: 0:05:57.\n",
            "  Batch   360  of    448.    Elapsed: 0:06:42.\n",
            "  Batch   400  of    448.    Elapsed: 0:07:27.\n",
            "  Batch   440  of    448.    Elapsed: 0:08:12.\n",
            "\n",
            "  Average training loss: 0.82\n",
            "  Training epoch took: 0:08:21\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation Loss: 0.52\n",
            "  Validation took: 0:00:21\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    448.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    448.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    448.    Elapsed: 0:02:14.\n",
            "  Batch   160  of    448.    Elapsed: 0:02:59.\n",
            "  Batch   200  of    448.    Elapsed: 0:03:44.\n",
            "  Batch   240  of    448.    Elapsed: 0:04:29.\n",
            "  Batch   280  of    448.    Elapsed: 0:05:13.\n",
            "  Batch   320  of    448.    Elapsed: 0:05:58.\n",
            "  Batch   360  of    448.    Elapsed: 0:06:43.\n",
            "  Batch   400  of    448.    Elapsed: 0:07:28.\n",
            "  Batch   440  of    448.    Elapsed: 0:08:13.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0:08:22\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:21\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    448.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    448.    Elapsed: 0:01:30.\n",
            "  Batch   120  of    448.    Elapsed: 0:02:14.\n",
            "  Batch   160  of    448.    Elapsed: 0:02:59.\n",
            "  Batch   200  of    448.    Elapsed: 0:03:44.\n",
            "  Batch   240  of    448.    Elapsed: 0:04:29.\n",
            "  Batch   280  of    448.    Elapsed: 0:05:13.\n",
            "  Batch   320  of    448.    Elapsed: 0:05:58.\n",
            "  Batch   360  of    448.    Elapsed: 0:06:43.\n",
            "  Batch   400  of    448.    Elapsed: 0:07:28.\n",
            "  Batch   440  of    448.    Elapsed: 0:08:12.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0:08:21\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:21\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:26:07 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgujpcXqvEfL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "8e894174-61f8-4756-90c1-c9ae468590bb"
      },
      "source": [
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0:08:21</td>\n",
              "      <td>0:00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0:08:22</td>\n",
              "      <td>0:00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.33</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0:08:21</td>\n",
              "      <td>0:00:21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.82         0.52           0.85       0:08:21         0:00:21\n",
              "2               0.47         0.45           0.87       0:08:22         0:00:21\n",
              "3               0.33         0.45           0.88       0:08:21         0:00:21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn--55R72G6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "0fdf13ee-482f-468d-9eb4-21226e2d951d"
      },
      "source": [
        "% matplotlib inline\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks(range(1,EPOCHS+1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c3wBkpT3VVE",
        "colab_type": "text"
      },
      "source": [
        "## Generate predictions on dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yaNixcy29Cg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7454785d-1be1-4494-c3ba-53fbd12c6f29"
      },
      "source": [
        "# Load the dataset into a pandas dataframe.\n",
        "dev_df = pd.read_csv(\n",
        "    \"datasets/dev_data.tsv\",\n",
        "    sep = \"\\t\",\n",
        "    header = 0,\n",
        "    index_col = 0\n",
        ") \n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(dev_df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = dev_df.text.values\n",
        "labels = le.transform(dev_df[THESE_LABELS])\n",
        "\n",
        "dev_df[THESE_LABELS+\"_encoded\"] = labels"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 2,850\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBdCRyMe3oXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "1f350e96-aa54-4ce6-ed23-38d7e3768f95"
      },
      "source": [
        "dev_df.sample(3)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>label_binary</th>\n",
              "      <th>rand_letter</th>\n",
              "      <th>text</th>\n",
              "      <th>label_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1211</th>\n",
              "      <td>Slogans</td>\n",
              "      <td>Propaganda</td>\n",
              "      <td>s</td>\n",
              "      <td>We will not be intimidated</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>545</th>\n",
              "      <td>No_Propaganda</td>\n",
              "      <td>No_Propaganda</td>\n",
              "      <td>s</td>\n",
              "      <td>Clinton regime, are unwilling to accept limita...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2068</th>\n",
              "      <td>No_Propaganda</td>\n",
              "      <td>No_Propaganda</td>\n",
              "      <td>s</td>\n",
              "      <td>Ivanka Trump\\nEmail *\\nPhone This field is for...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              label  ... label_encoded\n",
              "1211        Slogans  ...            12\n",
              "545   No_Propaganda  ...            10\n",
              "2068  No_Propaganda  ...            10\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_YOq_cM34gK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d923290b-b063-40dc-deeb-ff89fc0f5e4d"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    #`encode_plus` will:\n",
        "    #(1) Tokenize the sentence.\n",
        "    #(2) Prepend the `[CLS]` token to the start.\n",
        "    #(3) Append the `[SEP]` token to the end.\n",
        "    #(4) Map tokens to their IDs.\n",
        "    #(5) Pad or truncate the sentence to `max_length`\n",
        "    #(6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        sent,                         #Sentence to encode.\n",
        "        add_special_tokens = True,    #Add '[CLS]' and '[SEP]'\n",
        "        max_length = max_len,         #Pad & truncate all sentences.\n",
        "        truncation = True,\n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True, #Construct attn. masks.\n",
        "        return_tensors = 'pt',        #Return pytorch tensors.\n",
        "    )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler = prediction_sampler, batch_size = BATCH_SIZE)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiS6ksJX5BOB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b3e0369f-dfcd-4971-e5ee-a3d861edceb3"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions, true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, attention_mask = b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 2,850 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZyRBP2U5TqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XAjc_0g5nIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_probs = softmax(flat_predictions)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl-CzwvJ5vk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cfeb9217-b88e-4626-fc0b-b9065e2525bc"
      },
      "source": [
        "np.max(predicted_probs, axis=1)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.3257012e-04, 7.1852128e-07, 3.0596609e-06, ..., 5.8322720e-04,\n",
              "       6.4537115e-04, 5.1839574e-04], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw9gWRpr6Qyu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "a780f3e4-ff60-45ec-a544-1fc3616a7397"
      },
      "source": [
        ""
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-156-f896b21ebd24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtake_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mtake_along_axis\u001b[0;34m(arr, indices, axis)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# use the fancy index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_make_along_axis_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36m_make_along_axis_idx\u001b[0;34m(arr_shape, indices, axis)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# compute dimensions to iterate over\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`indices` must be an integer array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         raise ValueError(\n",
            "\u001b[0;31mIndexError\u001b[0m: `indices` must be an integer array"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owXb96DB61Dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}